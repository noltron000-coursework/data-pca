{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Make School Logo](./img/ms-logo.png)\n",
    "\n",
    "# Principal Component Analysis & Clustering\n",
    "\n",
    "For this notebook, we'll explore add two important techniques to our Data Science toolbox: **Principal Component Analysis (PCA)** and **Clustering**. Unlike all the Supervised Learning techniques we've used to far, these two are unique because they are examples of **Unsupervised Learning**. Whereas we require labeled data to double check the accuracy of algorithms like Decision Trees and Naive Bayesian Classifiers, these techniques work on unlabeled data. While this makes it much easier to apply these techniques to many more kinds of data, it also means that we have no way to measure how well the algorithm is or isn't working.\n",
    "\n",
    "## `CHALLENGE 1` Apply PCA to the *Iris Dataset*\n",
    "To help us explore the concept of PCA, we're going to start by applying PCA to the *Iris Dataset*. We'll then use it to fit a model and classify the flower types as we have done in previous examples.\n",
    "There are two prerequisite media posts to check out before we begin:\n",
    "- Watch this [primer from StatQuest](https://www.youtube-nocookie.com/embed/_UVHneBUBW0) to understand what PCA is.\n",
    "- Read this [short interactive article](http://setosa.io/ev/principal-component-analysis/) about using PCA on the *Iris Dataset*.\n",
    "\n",
    "These two examples should help you better understand how PCA works, and more importantly, how it can be useful to you.\n",
    "\n",
    "### Getting Started\n",
    "For our first challenge, we'll import the *Iris Dataset* from `sklearn.datasets` and use PCA on it. By examining the explained variance of Principal Components, we'll see that we can actually drop 1 or 2 columns (reducing our **dimensionality**) while only losing a minimal amount of predictive accuracy.\n",
    "\n",
    "Follow these steps in the code block below:\n",
    "1. Call `load_iris()` and store the results in the `iris` variable.\n",
    "1. Create a `StandardScaler()` object and store it in `scaler`.\n",
    "1. Call `scaler.fit()` on `iris.data`, and then use `scaler.transform` to create a scaled version of your data. Store the results in `scaled_x`.\n",
    "1. Store the labels for iris `labels`.\n",
    "1. Create a `PCA()` object and store it in `pca`. Fit it to the scaled data using `pca.fit()`. Then, call `pca.transform()` on `scaled_x` and store the results in `X_with_pca`.\n",
    "1. Complete the `enumerate` statement to to enumerate through `pca.explained_variance_ratio_` and print out the variance captured by each of the Principal Components.\n",
    "\n",
    "If you follow these steps correctly, you will create 4 *Principal Components* from the *Iris Dataset*. Be sure to use the information printed out by running the cell below to answer the following questions below it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports --\n",
    "# numpy for math stuff\n",
    "# pandas for data stuff\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "# needed for first section\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# get iris dataset from sklearn.datasets\n",
    "iris_json = load_iris()\n",
    "print(iris_json.DESCR)\n",
    "\n",
    "# create a pandas dataframe from the data\n",
    "iris = pandas.DataFrame(iris_json['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3\n",
       "0  5.1  3.5  1.4  0.2\n",
       "1  4.9  3.0  1.4  0.2\n",
       "2  4.7  3.2  1.3  0.2\n",
       "3  4.6  3.1  1.5  0.2\n",
       "4  5.0  3.6  1.4  0.2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the dataframe!\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay, now add in some column names...\n",
    "iris.columns = iris_json.feature_names\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the \"standard scaler\"\n",
    "# ==TODO==\n",
    "# define \"standard scaler\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to iris_json.data\n",
    "scaler.fit(iris)\n",
    "\n",
    "# call scaler.transform() on iris.data and store the result in iris_scaled_x\n",
    "iris_scaled_x = scaler.transform(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance for Principal Component 0: 0.7296244541329985\n",
      "Explained Variance for Principal Component 1: 0.22850761786701793\n",
      "Explained Variance for Principal Component 2: 0.036689218892828786\n",
      "Explained Variance for Principal Component 3: 0.005178709107154802\n"
     ]
    }
   ],
   "source": [
    "# get the PCA\n",
    "pca = PCA()\n",
    "\n",
    "# fit the pca object to iris_scaled_x\n",
    "pca.fit(iris_scaled_x)\n",
    "\n",
    "# enumerate through \"iris_pca.explained_variance_ratio_\"\n",
    "# to see the amount of variance captured by each Principal Component\n",
    "for ind, var in enumerate(pca.explained_variance_ratio_):\n",
    "\tprint(f'Explained Variance for Principal Component {ind}: {var}')\n",
    "\n",
    "# Call pca.transform() on scaled_X and store the results below\n",
    "iris_pca_x = pca.transform(iris_scaled_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding our Results\n",
    "#### Deducting Variance\n",
    "Based on the explained variances in the output above, do you recommend dropping any of the principal components to reduce dimensionality? Explain your answer.\n",
    "\n",
    "> *Answer will be here.*\n",
    "\n",
    "### Challenge: Fit a model using using Principal Components\n",
    "Using the data from above, complete the following steps:\n",
    "1. Import your PCA data into a dataframe. Name the columns `PC1`, `PC2`, `PC3`, and `PC4`.\n",
    "1. Drop `PC3` and `PC4` columns.\n",
    "1. Split your scaled data (currently stored in `scaled_X` and `labels`) into training and testing data using `train_test_split()`.\n",
    "1. Split your PCA data (currently stored in `X_with_pca` and `labels`) into training and testing sets using `train_test_split()`\n",
    "1. Create two `DecisionTreeClassifier` objects. Store one in `pca_clf` and one in `reg_clf`.\n",
    "1. Fit each model on their respective datasets, and make predictions from each. Compare the accuracy of each. Was the performance of the model fitted using the 2-dimensional PCA data of comparable performance? How would you tell.\n",
    "\n",
    "**Stretch Challenge:** Use `K-Fold Cross Validation` on each to run the models multiple times and get an average performance for each. Try this with K >= 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-142f6b1cb550>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-142f6b1cb550>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    pca_pred None\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pca_X_train, pca_X_test, pca_y_train, pca_y_test = None\n",
    "reg_X_train, reg_X_test, reg_y_train, reg_y_test = None\n",
    "\n",
    "clf = None\n",
    "clf_for_pca = None\n",
    "\n",
    "# Fit both models on the appropriate datasets\n",
    "\n",
    "# Use each fitted model to make predictions on the appropriate test sets\n",
    "reg_pred = None\n",
    "pca_pred None\n",
    "\n",
    "print(\"Accuracy for regular model: {}\".format(accuracy_score(reg_y_test, reg_pred)))\n",
    "print(\"Accuracy for model with PCA: {}\".format(accuracy_score(pca_y_test, pca_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "### TASK: Answer the following questions about PCA based on what you learned from class, the video, and the reading listed above.\n",
    "\n",
    "**How would you explain how PCA works to someone non-technical?**\n",
    "Answer:\n",
    "\n",
    "**In what way(s) can PCA be useful in Data Science and Machine Learning? Provide at least 2 examples.**\n",
    "Answer:\n",
    "\n",
    "### Challenge: Apply PCA and Clustering to Wholesale Customer Data\n",
    "In this notebook, we'll examine the [**Wholesale Customers Dataset**](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers), which we'll get from the UCI Machine Learning Datasets repository. This dataset contains the purchase records from clients of a wholesale distributor. It details the total annual purchases across categories seen in the data dictionary below:\n",
    "\n",
    "**Category** | **Description**\n",
    ":-----:|:-----:\n",
    "CHANNEL| 1= Hotel/Restaurant/Cafe, 2=Retailer (Nominal)\n",
    "REGION| Geographic region of Portugal for each order (Nominal)\n",
    "FRESH| Annual spending (m.u.) on fresh products (Continuous);\n",
    "MILK| Annual spending (m.u.) on milk products (Continuous);\n",
    "GROCERY| Annual spending (m.u.)on grocery products (Continuous);\n",
    "FROZEN| Annual spending (m.u.)on frozen products (Continuous)\n",
    "DETERGENTS\\_PAPER| Annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "DELICATESSEN| Annual spending (m.u.)on and delicatessen products (Continuous);\n",
    "\n",
    "**TASK:** Read in `wholesale_customers_data.csv` from the `datasets` folder and store in a dataframe. Store the `Channel` column in a separate variable, and then drop the `Channel` and `Region` columns from the dataframe. Scale the data and use PCA to engineer new features (Principal Components). Print out the explained variance for each principal component. Be sure to make your code portable--we'll be using this in our next Jupyter Notebook on K-Means Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "channel = None\n",
    "\n",
    "# Now Drop the Channel and Region Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
