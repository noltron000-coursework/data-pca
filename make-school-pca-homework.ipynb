{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Unsupervised Learning: Principal Component Analysis & Clustering</h1></center>\n",
    "\n",
    "For this notebook, we'll explore add two important techniques to our Data Science toolbox: **_Principal Component Analysis (PCA)_** and **_Clustering_**.  Unlike all the Supervised Learning techniques we've used to far, these two are unique because they are examples of **_Unsupervised Learning_**.  Whereas we require labeled data to double check the accuracy of algorithms like Decision Trees and Naive Bayesian Classifiers, these techniques work on unlabeled data.  While this makes it much easier to apply these techniques to many more kinds of data, it also means that we have no way to measure how well the algorithm is or isn't working.  \n",
    "\n",
    "\n",
    "<center><h3>Challenge: Apply PCA to the Iris Data Set</h3></center> \n",
    "\n",
    "To help us explore the concept of PCA, we're going to start by applying PCA to the _Iris Dataset_.  We'll then use it to fit a model and classify the flower types as we have done in previous examples.\n",
    "\n",
    "\n",
    " Before we begin, watch this primer from StatQuest to understand what PCA, and read this [short (interactive) article](http://setosa.io/ev/principal-component-analysis/) about using PCA on the Iris dataset.  These two examples should help you better understand how PCA works, and more importantly, how it can be useful to you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_UVHneBUBW0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_UVHneBUBW0\" \\\n",
    "frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Challenge: Apply PCA to the Iris Dataset</h3></center>\n",
    "\n",
    "For our first challenge, we'll import the _Iris Dataset_ from `sklearn.datasets` and use PCA on it. By examining the explained variance of Principal Components, we'll see that we can actually drop 1 or 2 columns (reducing our **_dimensionality_**) while only losing a minimal amount of predictive accuracy.  \n",
    "\n",
    "Follow these steps in the code block below:\n",
    "\n",
    "1. Call `load_iris()` and store the results in the `iris` variable. \n",
    "<br>\n",
    "<br>\n",
    "1. Create a `StandardScaler()` object and store it in `scaler`.\n",
    "<br>\n",
    "<br>\n",
    "1. Call `scaler.fit()` on `iris.data`, and then use `scaler.transform` to create a scaled version of your data.  Store the results in `scaled_x`.\n",
    "<br>\n",
    "<br>\n",
    "1. Store the labels for _iris_ `labels`.\n",
    "<br>\n",
    "<br>\n",
    "1.  Create a `PCA()` object and store it in `pca`.  Fit it to the scaled data using `pca.fit()`.  Then, call `pca.transform()` on `scaled_x` and store the results in `X_with_pca`.\n",
    "<br>\n",
    "<br>\n",
    "1. Complete the `enumerate` statement to to enumerate through `pca.explained_variance_ratio_` and print out the variance captured by each of the Principal Components.\n",
    "\n",
    "If you followed these steps correctly, you have will have now created 4 _Principal Components_ from your original dataset.  Be sure to use the information printed out by running the cell below to answer the following questions below it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = None\n",
    "scaler = None\n",
    "#Fit the scaler to iris.data\n",
    "\n",
    "# call scaler.transform() on iris.data and store the result in scaled_X\n",
    "scaled_X = None\n",
    "\n",
    "# Store the labels contained in iris.targets below\n",
    "labels = None\n",
    "\n",
    "# Create a PCA() object\n",
    "pca = None\n",
    "\n",
    "#Fit the pca object to scaled_X\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "# Call pca.transform() on scaled_X and store the results below\n",
    "X_with_pca = pca.transform(scaled_X)\n",
    "\n",
    "# Enumerate through pca.explained_variance_ratio_ to see the amount of variance captured by each Principal Component\n",
    "for ind, var in enumerate(None):\n",
    "    print(\"Explained Variance for Principal Component {}: {}\".format(ind, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Understanding our Results</h3></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>**_Challenge: Use your results from above to answer the following questions._**</center>\n",
    "<br>\n",
    "<br>\n",
    "<center>**_1.)Complete the following table using your results from above. _**</center>\n",
    "\n",
    "| Principal Component | Variance Explained  |\n",
    "|---------------------|---------------------|\n",
    "|      PC1               |                     |\n",
    "|      PC2            |                     |\n",
    "|      PC3               |                     |\n",
    "|      PC4            |                     |\n",
    "\n",
    "<center>**_2.) Based on the explained variances in the table above, do you recommend dropping any of the columns to reduce dimensionality? Explain your answer._**</center>\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "<center><h3>Challenge: Fit a model using using Principal Components</h3></center>\n",
    "\n",
    "Using the data from above, complete the following steps:\n",
    "\n",
    "1.  Import your PCA data into a dataframe. Name the columns `PC1`, `PC2`, `PC3`, and `PC4`.\n",
    "1.  Drop `PC3` and `PC4` columns.\n",
    "1.  Split your scaled data (currently stored in `scaled_X` and `labels`) into training and testing data using `train_test_split()`.\n",
    "1.  Split your PCA data (currently stored in `X_with_pca` and `labels`) into training and testing sets using `train_test_split()`\n",
    "1.  Create two `DecisionTreeClassifier` objects.  Store one in `pca_clf` and one in `reg_clf`.\n",
    "1.  Fit each model on their respective datasets, and make predictions from each.  Compare the accuracy of each. Was the performance of the model fitted using the 2-dimensional PCA data of comparable performance? How would you tell.  \n",
    "\n",
    "**_Stretch Challenge:_** Use `K-Fold Cross Validation` on each to run the models multiple times and get an average performance for each.  Try this with K >= 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train, pca_X_test, pca_y_train, pca_y_test = None\n",
    "reg_X_train, reg_X_test, reg_y_train, reg_y_test = None\n",
    "\n",
    "clf = None\n",
    "clf_for_pca = None\n",
    "\n",
    "# Fit both models on the appropriate datasets\n",
    "\n",
    "\n",
    "# Use each fitted model to make predictions on the appropriate test sets\n",
    "reg_pred = None\n",
    "pca_pred None\n",
    "\n",
    "print(\"Accuracy for regular model: {}\".format(accuracy_score(reg_y_test, reg_pred)))\n",
    "print(\"Accuracy for model with PCA: {}\".format(accuracy_score(pca_y_test, pca_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>What is PCA?</h3></center>\n",
    "\n",
    "**_TASK:_** Answer the following questions about PCA based on what you learned from class, the video, and the reading listed above. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>**_ How would you explain how PCA works to someone non-technical?_**</center>\n",
    "<br>\n",
    "Answer:\n",
    "<br>\n",
    "<br>\n",
    "<center>**_In what way(s) can PCA be useful in Data Science and Machine Learning? Provide at least 2 examples._**</center>\n",
    "<br>\n",
    "Answer:\n",
    "<br>\n",
    "<center><h3>Challenge: Apply PCA and Clustering to Wholesale Customer Data</h3></center>\n",
    "\n",
    "In this notebook, we'll examine the [**_Wholesale Customers Dataset_**](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers), which we'll get from the UCI Machine Learning Datasets repository.  This dataset contains the purchase records from clients of a wholesale distributor.  It details the total annual purchases across categories seen in the data dictionary below:\n",
    "\n",
    "**Category** | **Description** \n",
    ":-----:|:-----:\n",
    "CHANNEL| 1= Hotel/Restaurant/Cafe, 2=Retailer (Nominal)\n",
    "REGION| Geographic region of Portugal for each order (Nominal)\n",
    "FRESH| Annual spending (m.u.) on fresh products (Continuous);\n",
    "MILK| Annual spending (m.u.) on milk products (Continuous); \n",
    "GROCERY| Annual spending (m.u.)on grocery products (Continuous); \n",
    "FROZEN| Annual spending (m.u.)on frozen products (Continuous) \n",
    "DETERGENTS\\_PAPER| Annual spending (m.u.) on detergents and paper products (Continuous) \n",
    "DELICATESSEN| Annual spending (m.u.)on and delicatessen products (Continuous); \n",
    "\n",
    "**_TASK:_** Read in `wholesale_customers_data.csv` from the `datasets` folder and store in a dataframe.  Store the `Channel` column in a separate variable, and then drop the `Channel` and `Region` columns from the dataframe. Scale the data and use PCA to engineer new features (Principal Components).  Print out the explained variance for each principal component.  Be sure to make your code portable--we'll be using this in our next Jupyter Notebook on K-Means Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "channel = None\n",
    "\n",
    "# Now Drop the Channel and Region Columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
